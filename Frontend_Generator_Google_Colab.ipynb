{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f5d1385",
   "metadata": {},
   "source": [
    "# ☁️ React Frontend Generator - Google Colab\n",
    "\n",
    "**Run the AI-powered React Frontend Generator in Google Colab with GPU acceleration and Google Drive storage.**\n",
    "\n",
    "This notebook is optimized for Google Colab environment with automatic Google Drive integration for model caching and project storage.\n",
    "\n",
    "## 🎯 What You'll Get\n",
    "\n",
    "- **Complete React Applications**: Generated using local models or OpenAI API\n",
    "- **Modern TypeScript**: Best practices and clean code  \n",
    "- **Production Ready**: Includes package.json, testing, and build configs\n",
    "- **Google Drive Integration**: Models and projects saved to your Drive\n",
    "- **GPU Acceleration**: Utilizes Colab's free GPU resources\n",
    "- **Easy Download**: One-click download of generated projects\n",
    "\n",
    "## ☁️ Google Colab Advantages\n",
    "\n",
    "- **Free GPU Access**: T4 GPU for faster model inference\n",
    "- **No Local Setup**: Everything runs in the cloud\n",
    "- **Google Drive Storage**: Persistent model caching across sessions\n",
    "- **Easy Sharing**: Share notebooks with team members\n",
    "- **Pre-installed Libraries**: Most dependencies already available\n",
    "\n",
    "## 📋 Requirements\n",
    "\n",
    "- **Google Account**: For Colab and Drive access\n",
    "- **Optional**: OpenAI API key (for API-based generation)\n",
    "- **Storage**: ~5-15GB Google Drive space for models\n",
    "\n",
    "## 🚀 Quick Start\n",
    "\n",
    "1. **Mount Google Drive**: Persistent storage for models and projects\n",
    "2. **Choose Model Type**: Local models (free) or OpenAI API (paid)\n",
    "3. **Download Repository**: Get the latest Frontend Generator code\n",
    "4. **Generate React App**: From simple todo to enterprise platforms\n",
    "5. **Download Results**: ZIP file ready for local development\n",
    "\n",
    "Let's build amazing React apps in the cloud! ☁️🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306bbbc2",
   "metadata": {},
   "source": [
    "## 🔧 Step 1: Colab Environment Setup\n",
    "\n",
    "Install dependencies and check Colab GPU resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cccc2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab environment setup\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"☁️ Setting up Google Colab environment...\")\n",
    "\n",
    "# Install required packages for Colab\n",
    "!pip install -q vllm transformers torch tqdm\n",
    "\n",
    "# Optional: OpenAI for API-based generation\n",
    "!pip install -q openai\n",
    "\n",
    "# Check Colab GPU resources\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "print(\"\\n🔍 Colab Environment Analysis:\")\n",
    "\n",
    "# GPU Information\n",
    "if torch.cuda.is_available():\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"🎮 GPU Available: ✅ ({gpu_count} device{'s' if gpu_count > 1 else ''})\")\n",
    "    \n",
    "    for i in range(gpu_count):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "        print(f\"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB VRAM)\")\n",
    "else:\n",
    "    print(\"🎮 GPU Available: ❌ (Using CPU mode)\")\n",
    "\n",
    "# Memory Information\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=memory.total,memory.free', '--format=csv,noheader,nounits'], \n",
    "                          capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        for i, line in enumerate(result.stdout.strip().split('\\n')):\n",
    "            total, free = line.split(', ')\n",
    "            print(f\"   GPU {i} Memory: {free}MB free / {total}MB total\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# System Memory\n",
    "import psutil\n",
    "memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "available_gb = psutil.virtual_memory().available / (1024**3)\n",
    "\n",
    "print(f\"💾 System RAM: {available_gb:.1f}GB available / {memory_gb:.1f}GB total\")\n",
    "\n",
    "# Disk Space\n",
    "disk = psutil.disk_usage('/')\n",
    "disk_free_gb = disk.free / (1024**3)\n",
    "print(f\"💽 Disk Space: {disk_free_gb:.1f}GB free\")\n",
    "\n",
    "# Recommend model based on Colab resources\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    if gpu_memory >= 14:  # High-end GPU (A100, etc.)\n",
    "        recommended_model = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "        print(f\"🎯 Recommended: Large model ({recommended_model})\")\n",
    "    elif gpu_memory >= 8:  # Standard Colab GPU (T4)\n",
    "        recommended_model = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "        print(f\"🎯 Recommended: Medium model ({recommended_model})\")\n",
    "    else:  # Lower VRAM\n",
    "        recommended_model = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
    "        print(f\"🎯 Recommended: Small model ({recommended_model})\")\n",
    "else:\n",
    "    recommended_model = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
    "    print(f\"🎯 Recommended: CPU model ({recommended_model})\")\n",
    "\n",
    "print(\"\\n☁️ Colab environment ready!\")\n",
    "print(\"💡 Consider using OpenAI API for consistent performance across sessions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec6374d",
   "metadata": {},
   "source": [
    "## 💾 Step 2: Google Drive Integration\n",
    "\n",
    "Mount Google Drive for persistent model caching and project storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e6940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent storage\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "print(\"💾 Mounting Google Drive...\")\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Create organized directory structure in Google Drive\n",
    "DRIVE_PATH = \"/content/drive/MyDrive\"\n",
    "WORK_DIR = f\"{DRIVE_PATH}/Frontend_Generator\"\n",
    "MODELS_DIR = f\"{WORK_DIR}/models\"\n",
    "PROJECTS_DIR = f\"{WORK_DIR}/generated_projects\"\n",
    "CACHE_DIR = f\"{WORK_DIR}/cache\"\n",
    "\n",
    "# Create directories\n",
    "directories = [WORK_DIR, MODELS_DIR, PROJECTS_DIR, CACHE_DIR]\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f\"📁 Created: {directory}\")\n",
    "\n",
    "# Change to working directory\n",
    "os.chdir(WORK_DIR)\n",
    "\n",
    "print(f\"\\n✅ Google Drive mounted successfully!\")\n",
    "print(f\"🏠 Working directory: {WORK_DIR}\")\n",
    "print(f\"🤖 Models cache: {MODELS_DIR}\")\n",
    "print(f\"📱 Projects: {PROJECTS_DIR}\")\n",
    "print(f\"📂 Current location: {os.getcwd()}\")\n",
    "\n",
    "# Check Drive space\n",
    "drive_space = os.statvfs(DRIVE_PATH)\n",
    "free_space_gb = (drive_space.f_frsize * drive_space.f_avail) / (1024**3)\n",
    "print(f\"💽 Google Drive free space: {free_space_gb:.1f}GB\")\n",
    "\n",
    "if free_space_gb < 10:\n",
    "    print(\"⚠️ Warning: Low Google Drive space. Models require 5-15GB.\")\n",
    "    print(\"💡 Consider cleaning up Drive or using OpenAI API instead\")\n",
    "else:\n",
    "    print(\"✅ Sufficient Drive space for model caching\")\n",
    "\n",
    "print(\"\\n🔄 Google Drive integration complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
