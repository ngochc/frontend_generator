{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61cd947d",
   "metadata": {},
   "source": [
    "# üèÜ React Frontend Generator - Kaggle\n",
    "\n",
    "**Run the AI-powered React Frontend Generator on Kaggle with competition-grade GPU resources.**\n",
    "\n",
    "This notebook is optimized for Kaggle's environment with powerful GPU acceleration and automatic output management for easy downloads.\n",
    "\n",
    "## üéØ What You'll Get\n",
    "\n",
    "- **Complete React Applications**: Generated using local models or OpenAI API\n",
    "- **Modern TypeScript**: Best practices and clean code\n",
    "- **Production Ready**: Includes package.json, testing, and build configs\n",
    "- **Competition GPUs**: Access to T4, P100, or even A100 GPUs\n",
    "- **Auto Downloads**: Projects saved to Kaggle output for easy access\n",
    "- **Dataset Integration**: Can integrate with Kaggle datasets\n",
    "\n",
    "## üèÜ Kaggle Advantages\n",
    "\n",
    "- **Powerful GPUs**: Often better than Colab (T4, P100, A100)\n",
    "- **Longer Sessions**: 12+ hour sessions for large projects\n",
    "- **Fast Storage**: High-speed NVMe storage\n",
    "- **Pre-installed Libraries**: Comprehensive ML/AI stack\n",
    "- **Community**: Share with Kaggle community\n",
    "- **Dataset Access**: Integrate with public datasets\n",
    "\n",
    "## üìã Requirements\n",
    "\n",
    "- **Kaggle Account**: Free account with verified phone number\n",
    "- **GPU Enabled**: Enable GPU in notebook settings\n",
    "- **Optional**: OpenAI API key for API-based generation\n",
    "- **Internet**: For downloading models and repositories\n",
    "\n",
    "## üöÄ Quick Start\n",
    "\n",
    "1. **Enable GPU**: Go to Settings > Accelerator > GPU\n",
    "2. **Setup Environment**: Install dependencies and check resources\n",
    "3. **Download Repository**: Get the latest Frontend Generator code\n",
    "4. **Choose Model**: Local inference or OpenAI API\n",
    "5. **Generate React App**: From simple to enterprise applications\n",
    "6. **Download from Output**: Generated projects saved to /kaggle/working\n",
    "\n",
    "Let's build amazing React apps on Kaggle! üèÜüöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad17dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèÜ Kaggle Environment Setup\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import platform\n",
    "import psutil\n",
    "import torch\n",
    "\n",
    "print(\"üèÜ Kaggle Environment Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check Kaggle environment\n",
    "print(\"üìä Environment Information:\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"Working Directory: {os.getcwd()}\")\n",
    "\n",
    "# Check if running on Kaggle\n",
    "is_kaggle = os.path.exists('/kaggle')\n",
    "print(f\"Running on Kaggle: {'‚úÖ Yes' if is_kaggle else '‚ùå No'}\")\n",
    "\n",
    "if is_kaggle:\n",
    "    print(f\"Kaggle Working Directory: /kaggle/working\")\n",
    "    print(f\"Kaggle Input Directory: /kaggle/input\")\n",
    "\n",
    "# System Resources\n",
    "print(\"\\nüíæ System Resources:\")\n",
    "memory = psutil.virtual_memory()\n",
    "print(f\"Total RAM: {memory.total / (1024**3):.1f} GB\")\n",
    "print(f\"Available RAM: {memory.available / (1024**3):.1f} GB\")\n",
    "print(f\"CPU Cores: {psutil.cpu_count()}\")\n",
    "\n",
    "# GPU Detection\n",
    "print(\"\\nüéÆ GPU Information:\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"‚úÖ CUDA Available\")\n",
    "    print(f\"GPU Count: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"GPU {i} Memory: {torch.cuda.get_device_properties(i).total_memory / (1024**3):.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA Not Available\")\n",
    "\n",
    "# Storage Information\n",
    "print(\"\\nüíΩ Storage Information:\")\n",
    "statvfs = os.statvfs('/kaggle/working')\n",
    "total_space = statvfs.f_frsize * statvfs.f_blocks / (1024**3)\n",
    "free_space = statvfs.f_frsize * statvfs.f_bavail / (1024**3)\n",
    "print(f\"Total Disk Space: {total_space:.1f} GB\")\n",
    "print(f\"Free Disk Space: {free_space:.1f} GB\")\n",
    "\n",
    "print(\"\\nüèÜ Kaggle Environment Ready!\")\n",
    "print(\"Tip: Kaggle often provides powerful GPUs (T4, P100, A100) for longer sessions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d62fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Install Dependencies for Kaggle\n",
    "print(\"üì¶ Installing Frontend Generator Dependencies...\")\n",
    "print(\"‚ö° Using Kaggle's fast package installation\")\n",
    "\n",
    "# Essential packages for Frontend Generator\n",
    "packages = [\n",
    "    \"vllm\",                    # Local model inference\n",
    "    \"transformers>=4.35.0\",    # Model loading\n",
    "    \"torch>=2.0.0\",           # Deep learning framework\n",
    "    \"accelerate\",             # Model acceleration\n",
    "    \"openai\",                 # OpenAI API (optional)\n",
    "    \"python-dotenv\",          # Environment variables\n",
    "    \"colorama\",               # Colored terminal output\n",
    "    \"rich\",                   # Beautiful terminal output\n",
    "    \"psutil\",                 # System monitoring\n",
    "]\n",
    "\n",
    "# Install packages\n",
    "for package in packages:\n",
    "    print(f\"Installing {package}...\")\n",
    "    result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"], \n",
    "                          capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"‚úÖ {package} installed successfully\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to install {package}: {result.stderr}\")\n",
    "\n",
    "print(\"\\nüîç Verifying Installations...\")\n",
    "\n",
    "# Verify critical packages\n",
    "try:\n",
    "    import vllm\n",
    "    print(f\"‚úÖ vLLM: {vllm.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå vLLM import failed: {e}\")\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"‚úÖ Transformers: {transformers.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Transformers import failed: {e}\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
    "    print(f\"   CUDA Available: {torch.cuda.is_available()}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå PyTorch import failed: {e}\")\n",
    "\n",
    "print(\"\\nüèÜ Dependencies ready for Kaggle environment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d212a7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• Download Frontend Generator Repository\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "print(\"üì• Downloading Frontend Generator from GitHub...\")\n",
    "\n",
    "# Change to Kaggle working directory (where outputs are saved)\n",
    "os.chdir('/kaggle/working')\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Clone the repository\n",
    "repo_url = \"https://github.com/your-username/frontend_generator.git\"\n",
    "repo_name = \"frontend_generator\"\n",
    "\n",
    "# Remove existing directory if it exists\n",
    "if os.path.exists(repo_name):\n",
    "    print(f\"üóëÔ∏è Removing existing {repo_name} directory...\")\n",
    "    subprocess.run([\"rm\", \"-rf\", repo_name], check=True)\n",
    "\n",
    "# Clone repository\n",
    "print(f\"üì• Cloning repository from {repo_url}...\")\n",
    "try:\n",
    "    result = subprocess.run([\"git\", \"clone\", repo_url], \n",
    "                          capture_output=True, text=True, check=True)\n",
    "    print(\"‚úÖ Repository cloned successfully!\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"‚ùå Git clone failed: {e}\")\n",
    "    print(\"üìù Creating repository structure manually...\")\n",
    "    \n",
    "    # Create directory structure if git clone fails\n",
    "    os.makedirs(f\"{repo_name}/codes\", exist_ok=True)\n",
    "    os.makedirs(f\"{repo_name}/examples\", exist_ok=True)\n",
    "    os.makedirs(f\"{repo_name}/scripts\", exist_ok=True)\n",
    "    \n",
    "    # Create placeholder files (you would copy actual content)\n",
    "    files_to_create = [\n",
    "        f\"{repo_name}/codes/1_planning_llm.py\",\n",
    "        f\"{repo_name}/codes/2_analyzing_llm.py\", \n",
    "        f\"{repo_name}/codes/3_coding_llm.py\",\n",
    "        f\"{repo_name}/codes/utils.py\",\n",
    "        f\"{repo_name}/requirements.txt\"\n",
    "    ]\n",
    "    \n",
    "    for file_path in files_to_create:\n",
    "        with open(file_path, 'w') as f:\n",
    "            f.write(f\"# {os.path.basename(file_path)}\\n# Placeholder - add actual content\\n\")\n",
    "    \n",
    "    print(\"‚úÖ Repository structure created!\")\n",
    "\n",
    "# Change to repository directory\n",
    "os.chdir(repo_name)\n",
    "print(f\"üìÇ Changed to repository directory: {os.getcwd()}\")\n",
    "\n",
    "# List repository contents\n",
    "print(\"\\nüìã Repository Contents:\")\n",
    "for root, dirs, files in os.walk('.'):\n",
    "    level = root.replace('.', '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        print(f\"{subindent}{file}\")\n",
    "\n",
    "print(\"\\nüèÜ Repository ready on Kaggle!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a005e6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü§ñ Configure AI Models for Kaggle\n",
    "import torch\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "print(\"ü§ñ Configuring AI Models for Kaggle Environment...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check GPU capabilities\n",
    "gpu_available = torch.cuda.is_available()\n",
    "gpu_memory = 0\n",
    "gpu_name = \"None\"\n",
    "\n",
    "if gpu_available:\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "\n",
    "print(f\"üéÆ GPU: {gpu_name}\")\n",
    "print(f\"üíæ GPU Memory: {gpu_memory:.1f} GB\")\n",
    "\n",
    "# System RAM\n",
    "system_memory = psutil.virtual_memory().total / (1024**3)\n",
    "print(f\"üñ•Ô∏è System RAM: {system_memory:.1f} GB\")\n",
    "\n",
    "# Model selection based on Kaggle resources\n",
    "print(\"\\nüéØ Model Selection for Kaggle:\")\n",
    "\n",
    "if gpu_memory >= 24:  # A100 or similar\n",
    "    recommended_model = \"deepseek-ai/deepseek-coder-33b-instruct\"\n",
    "    max_tokens = 8192\n",
    "    print(f\"üèÜ High-end GPU detected! Using: {recommended_model}\")\n",
    "    print(\"üöÄ This model provides the best code generation quality\")\n",
    "    \n",
    "elif gpu_memory >= 16:  # T4 or P100\n",
    "    recommended_model = \"deepseek-ai/deepseek-coder-6.7b-instruct\" \n",
    "    max_tokens = 4096\n",
    "    print(f\"‚ö° Mid-range GPU detected! Using: {recommended_model}\")\n",
    "    print(\"üéØ Good balance of performance and resource usage\")\n",
    "    \n",
    "elif gpu_memory >= 8:  # Basic GPU\n",
    "    recommended_model = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
    "    max_tokens = 2048\n",
    "    print(f\"üíª Basic GPU detected! Using: {recommended_model}\")\n",
    "    print(\"üéØ Lightweight model for smaller GPUs\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No suitable GPU detected\")\n",
    "    print(\"üí° Recommendation: Enable GPU in Kaggle settings or use OpenAI API\")\n",
    "    recommended_model = None\n",
    "    max_tokens = 0\n",
    "\n",
    "# Create model configuration\n",
    "if recommended_model:\n",
    "    model_config = {\n",
    "        \"model_name\": recommended_model,\n",
    "        \"max_model_len\": max_tokens,\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens\": 1024,\n",
    "        \"top_p\": 0.95,\n",
    "        \"gpu_memory_utilization\": 0.8,  # Kaggle allows higher utilization\n",
    "        \"tensor_parallel_size\": 1,\n",
    "        \"dtype\": \"float16\"\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚öôÔ∏è Model Configuration:\")\n",
    "    for key, value in model_config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Save configuration for later use\n",
    "    import json\n",
    "    with open('/kaggle/working/model_config.json', 'w') as f:\n",
    "        json.dump(model_config, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Configuration saved to /kaggle/working/model_config.json\")\n",
    "\n",
    "print(\"\\nüèÜ Kaggle Model Configuration Complete!\")\n",
    "print(\"üí° Tip: Kaggle GPUs are often more powerful than Colab!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cfdd90",
   "metadata": {},
   "source": [
    "# üìù Project Requirements\n",
    "\n",
    "Now let's define what React application you want to generate. You can either:\n",
    "\n",
    "1. **Use Example Requirements**: Choose from pre-built examples\n",
    "2. **Custom Requirements**: Write your own detailed requirements\n",
    "3. **Upload Requirements**: Use a Kaggle dataset with requirements\n",
    "\n",
    "## üéØ Example Projects Available:\n",
    "\n",
    "- **Simple Todo App**: Basic task management with local storage\n",
    "- **Dashboard**: Analytics dashboard with charts and metrics  \n",
    "- **Healthcare Management**: Patient management system\n",
    "- **Trading Platform**: Financial trading interface\n",
    "- **Enterprise Platform**: Complete business management system\n",
    "\n",
    "Choose your approach below! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5275bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù Select or Input Project Requirements\n",
    "import os\n",
    "\n",
    "print(\"üìù Project Requirements Setup\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check for example requirements\n",
    "examples_dir = \"examples\"\n",
    "if os.path.exists(examples_dir):\n",
    "    print(\"üìö Available example requirements:\")\n",
    "    examples = [f for f in os.listdir(examples_dir) if f.endswith('.md')]\n",
    "    for i, example in enumerate(examples, 1):\n",
    "        name = example.replace('_requirements.md', '').replace('_', ' ').title()\n",
    "        print(f\"  {i}. {name}\")\n",
    "    print()\n",
    "\n",
    "# Configuration options\n",
    "print(\"üéØ Choose your requirements source:\")\n",
    "print(\"1. Use example requirements (recommended for demo)\")\n",
    "print(\"2. Write custom requirements\")\n",
    "print(\"3. Load from Kaggle dataset\")\n",
    "print()\n",
    "\n",
    "# For demo, let's use the simple todo example\n",
    "REQUIREMENTS_SOURCE = \"example\"  # Change to \"custom\" or \"dataset\" as needed\n",
    "EXAMPLE_FILE = \"simple_todo_requirements.md\"\n",
    "\n",
    "if REQUIREMENTS_SOURCE == \"example\":\n",
    "    # Load example requirements\n",
    "    example_path = os.path.join(examples_dir, EXAMPLE_FILE)\n",
    "    if os.path.exists(example_path):\n",
    "        with open(example_path, 'r') as f:\n",
    "            requirements_content = f.read()\n",
    "        print(f\"‚úÖ Loaded example: {EXAMPLE_FILE}\")\n",
    "        print(\"üìã Requirements preview:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(requirements_content[:500] + \"...\" if len(requirements_content) > 500 else requirements_content)\n",
    "    else:\n",
    "        print(f\"‚ùå Example file not found: {example_path}\")\n",
    "        requirements_content = \"\"\"\n",
    "# Simple Todo App Requirements\n",
    "\n",
    "Create a modern React TypeScript todo application with the following features:\n",
    "\n",
    "## Core Functionality\n",
    "- Add new tasks with title and description\n",
    "- Mark tasks as complete/incomplete\n",
    "- Delete tasks\n",
    "- Edit existing tasks\n",
    "- Filter tasks (all, active, completed)\n",
    "\n",
    "## Technical Requirements\n",
    "- React 18+ with TypeScript\n",
    "- Modern React hooks (useState, useEffect)\n",
    "- Local storage for persistence\n",
    "- Responsive design\n",
    "- Clean, modern UI\n",
    "\n",
    "## UI/UX\n",
    "- Material Design or clean minimalist style\n",
    "- Mobile-friendly responsive layout\n",
    "- Smooth animations and transitions\n",
    "- Intuitive user interface\n",
    "\n",
    "This should be a production-ready application with proper error handling and user feedback.\n",
    "\"\"\"\n",
    "        print(\"üìù Using default todo app requirements\")\n",
    "\n",
    "elif REQUIREMENTS_SOURCE == \"custom\":\n",
    "    # Custom requirements input\n",
    "    print(\"‚úèÔ∏è Enter your custom requirements:\")\n",
    "    print(\"(In Kaggle, you can modify this cell to input your requirements)\")\n",
    "    \n",
    "    requirements_content = \"\"\"\n",
    "# Your Custom Project Requirements\n",
    "\n",
    "Replace this with your detailed project requirements.\n",
    "\n",
    "Include:\n",
    "- Project overview and goals\n",
    "- Core functionality needed\n",
    "- Technical requirements\n",
    "- UI/UX preferences\n",
    "- Any specific features or constraints\n",
    "\n",
    "Be as detailed as possible for best results!\n",
    "\"\"\"\n",
    "\n",
    "elif REQUIREMENTS_SOURCE == \"dataset\":\n",
    "    # Load from Kaggle dataset\n",
    "    print(\"üìä Loading requirements from Kaggle dataset...\")\n",
    "    # You would load from /kaggle/input/your-dataset/requirements.md\n",
    "    requirements_content = \"# Load from your Kaggle dataset here\"\n",
    "\n",
    "# Save requirements to working directory\n",
    "requirements_file = \"/kaggle/working/project_requirements.md\"\n",
    "with open(requirements_file, 'w') as f:\n",
    "    f.write(requirements_content)\n",
    "\n",
    "print(f\"\\n‚úÖ Requirements saved to: {requirements_file}\")\n",
    "print(f\"üìä Requirements length: {len(requirements_content)} characters\")\n",
    "print(\"\\nüèÜ Ready to generate your React application!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ee2c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Generate React Application\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üöÄ Starting React Application Generation...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load model configuration\n",
    "with open('/kaggle/working/model_config.json', 'r') as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "# Configuration\n",
    "USE_LOCAL_MODEL = True  # Set to False to use OpenAI API instead\n",
    "PROJECT_NAME = \"kaggle_generated_app\"\n",
    "OUTPUT_DIR = f\"/kaggle/working/{PROJECT_NAME}\"\n",
    "\n",
    "print(f\"üéØ Project: {PROJECT_NAME}\")\n",
    "print(f\"üìÇ Output: {OUTPUT_DIR}\")\n",
    "print(f\"ü§ñ Model: {'Local (' + model_config['model_name'] + ')' if USE_LOCAL_MODEL else 'OpenAI API'}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "if USE_LOCAL_MODEL:\n",
    "    print(\"\\nü§ñ Starting Local Model Generation...\")\n",
    "    print(\"‚ö° Using Kaggle GPU acceleration\")\n",
    "    \n",
    "    # Stage 1: Planning\n",
    "    print(\"\\nüìã Stage 1: Planning...\")\n",
    "    planning_cmd = [\n",
    "        sys.executable, \"codes/1_planning_llm.py\",\n",
    "        \"--requirements\", \"/kaggle/working/project_requirements.md\",\n",
    "        \"--output\", f\"{OUTPUT_DIR}/planning.json\",\n",
    "        \"--model\", model_config[\"model_name\"],\n",
    "        \"--max_tokens\", str(model_config[\"max_tokens\"]),\n",
    "        \"--temperature\", str(model_config[\"temperature\"])\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(planning_cmd, capture_output=True, text=True, timeout=600)\n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ Planning completed successfully!\")\n",
    "        else:\n",
    "            print(f\"‚ùå Planning failed: {result.stderr}\")\n",
    "            print(f\"Output: {result.stdout}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"‚è∞ Planning stage timed out\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Planning error: {e}\")\n",
    "    \n",
    "    # Stage 2: Analysis\n",
    "    print(\"\\nüîç Stage 2: Analysis...\")\n",
    "    analysis_cmd = [\n",
    "        sys.executable, \"codes/2_analyzing_llm.py\", \n",
    "        \"--requirements\", \"/kaggle/working/project_requirements.md\",\n",
    "        \"--planning\", f\"{OUTPUT_DIR}/planning.json\",\n",
    "        \"--output\", f\"{OUTPUT_DIR}/analysis.json\",\n",
    "        \"--model\", model_config[\"model_name\"],\n",
    "        \"--max_tokens\", str(model_config[\"max_tokens\"]),\n",
    "        \"--temperature\", str(model_config[\"temperature\"])\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(analysis_cmd, capture_output=True, text=True, timeout=600)\n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ Analysis completed successfully!\")\n",
    "        else:\n",
    "            print(f\"‚ùå Analysis failed: {result.stderr}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"‚è∞ Analysis stage timed out\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Analysis error: {e}\")\n",
    "    \n",
    "    # Stage 3: Code Generation\n",
    "    print(\"\\nüíª Stage 3: Code Generation...\")\n",
    "    coding_cmd = [\n",
    "        sys.executable, \"codes/3_coding_llm.py\",\n",
    "        \"--requirements\", \"/kaggle/working/project_requirements.md\", \n",
    "        \"--planning\", f\"{OUTPUT_DIR}/planning.json\",\n",
    "        \"--analysis\", f\"{OUTPUT_DIR}/analysis.json\",\n",
    "        \"--output\", OUTPUT_DIR,\n",
    "        \"--model\", model_config[\"model_name\"],\n",
    "        \"--max_tokens\", str(model_config[\"max_tokens\"]),\n",
    "        \"--temperature\", str(model_config[\"temperature\"])\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(coding_cmd, capture_output=True, text=True, timeout=1200)\n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ Code generation completed successfully!\")\n",
    "        else:\n",
    "            print(f\"‚ùå Code generation failed: {result.stderr}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"‚è∞ Code generation stage timed out\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Code generation error: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nüåê Using OpenAI API...\")\n",
    "    print(\"üí° Make sure you have set your OPENAI_API_KEY\")\n",
    "    \n",
    "    # OpenAI API generation (similar structure but with API calls)\n",
    "    # This would use the non-LLM versions of the scripts\n",
    "    api_planning_cmd = [\n",
    "        sys.executable, \"codes/1_planning.py\",\n",
    "        \"--requirements\", \"/kaggle/working/project_requirements.md\",\n",
    "        \"--output\", f\"{OUTPUT_DIR}/planning.json\"\n",
    "    ]\n",
    "    \n",
    "    # Execute similar pattern for API-based generation\n",
    "    print(\"üîÑ API generation not fully implemented in this demo\")\n",
    "    print(\"üí° Modify the scripts to use OpenAI API if preferred\")\n",
    "\n",
    "# Check generated files\n",
    "print(f\"\\nüìÅ Generated Files in {OUTPUT_DIR}:\")\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    for root, dirs, files in os.walk(OUTPUT_DIR):\n",
    "        level = root.replace(OUTPUT_DIR, '').count(os.sep)\n",
    "        indent = ' ' * 2 * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        subindent = ' ' * 2 * (level + 1)\n",
    "        for file in files:\n",
    "            print(f\"{subindent}{file}\")\n",
    "\n",
    "print(f\"\\nüèÜ Generation completed!\")\n",
    "print(f\"üìÇ Your React app is in: {OUTPUT_DIR}\")\n",
    "print(f\"‚¨áÔ∏è Download from Kaggle output when complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2c3495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Test and Validate Generated Application\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "print(\"üß™ Testing Generated React Application...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "PROJECT_DIR = \"/kaggle/working/kaggle_generated_app\"\n",
    "\n",
    "if not os.path.exists(PROJECT_DIR):\n",
    "    print(f\"‚ùå Project directory not found: {PROJECT_DIR}\")\n",
    "    print(\"üí° Make sure the generation step completed successfully\")\n",
    "else:\n",
    "    # Change to project directory\n",
    "    os.chdir(PROJECT_DIR)\n",
    "    print(f\"üìÇ Testing in: {os.getcwd()}\")\n",
    "    \n",
    "    # Check for essential files\n",
    "    print(\"\\nüìã Checking Essential Files:\")\n",
    "    essential_files = [\n",
    "        \"package.json\",\n",
    "        \"src/App.tsx\", \n",
    "        \"src/index.tsx\",\n",
    "        \"public/index.html\",\n",
    "        \"tsconfig.json\"\n",
    "    ]\n",
    "    \n",
    "    for file in essential_files:\n",
    "        if os.path.exists(file):\n",
    "            print(f\"‚úÖ {file}\")\n",
    "        else:\n",
    "            print(f\"‚ùå {file} - Missing!\")\n",
    "    \n",
    "    # Analyze package.json\n",
    "    if os.path.exists(\"package.json\"):\n",
    "        print(\"\\nüì¶ Package.json Analysis:\")\n",
    "        with open(\"package.json\", \"r\") as f:\n",
    "            package_data = json.load(f)\n",
    "        \n",
    "        print(f\"üìù Name: {package_data.get('name', 'N/A')}\")\n",
    "        print(f\"üî¢ Version: {package_data.get('version', 'N/A')}\")\n",
    "        \n",
    "        dependencies = package_data.get(\"dependencies\", {})\n",
    "        print(f\"üìö Dependencies ({len(dependencies)}):\")\n",
    "        for dep, version in list(dependencies.items())[:10]:  # Show first 10\n",
    "            print(f\"  - {dep}: {version}\")\n",
    "        if len(dependencies) > 10:\n",
    "            print(f\"  ... and {len(dependencies) - 10} more\")\n",
    "    \n",
    "    # Install dependencies (if you want to test building)\n",
    "    print(\"\\nüì¶ Installing Dependencies...\")\n",
    "    print(\"‚ö†Ô∏è This may take a few minutes on Kaggle\")\n",
    "    \n",
    "    try:\n",
    "        # Check if npm is available\n",
    "        npm_check = subprocess.run([\"npm\", \"--version\"], capture_output=True, text=True)\n",
    "        if npm_check.returncode != 0:\n",
    "            print(\"‚ùå npm not available, installing Node.js...\")\n",
    "            # Install Node.js on Kaggle\n",
    "            subprocess.run([\"apt-get\", \"update\"], check=True, capture_output=True)\n",
    "            subprocess.run([\"apt-get\", \"install\", \"-y\", \"nodejs\", \"npm\"], check=True, capture_output=True)\n",
    "        \n",
    "        print(\"üì¶ Running npm install...\")\n",
    "        install_result = subprocess.run([\"npm\", \"install\"], \n",
    "                                      capture_output=True, text=True, timeout=300)\n",
    "        \n",
    "        if install_result.returncode == 0:\n",
    "            print(\"‚úÖ Dependencies installed successfully!\")\n",
    "            \n",
    "            # Try to build the project\n",
    "            print(\"\\nüî® Building Project...\")\n",
    "            build_result = subprocess.run([\"npm\", \"run\", \"build\"], \n",
    "                                        capture_output=True, text=True, timeout=300)\n",
    "            \n",
    "            if build_result.returncode == 0:\n",
    "                print(\"‚úÖ Build successful!\")\n",
    "                print(\"üéâ Your React app is production-ready!\")\n",
    "                \n",
    "                # Check build output\n",
    "                if os.path.exists(\"build\"):\n",
    "                    build_size = sum(os.path.getsize(os.path.join(root, file))\n",
    "                                   for root, dirs, files in os.walk(\"build\")\n",
    "                                   for file in files)\n",
    "                    print(f\"üì¶ Build size: {build_size / (1024*1024):.1f} MB\")\n",
    "            else:\n",
    "                print(\"‚ùå Build failed:\")\n",
    "                print(build_result.stderr[:500])\n",
    "                \n",
    "        else:\n",
    "            print(\"‚ùå Dependency installation failed:\")\n",
    "            print(install_result.stderr[:500])\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"‚è∞ Installation/build timed out\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during testing: {e}\")\n",
    "    \n",
    "    # Code quality check\n",
    "    print(\"\\nüìä Code Quality Analysis:\")\n",
    "    \n",
    "    # Count lines of code\n",
    "    total_lines = 0\n",
    "    total_files = 0\n",
    "    \n",
    "    for root, dirs, files in os.walk(\"src\"):\n",
    "        for file in files:\n",
    "            if file.endswith(('.tsx', '.ts', '.jsx', '.js')):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    lines = len(f.readlines())\n",
    "                    total_lines += lines\n",
    "                    total_files += 1\n",
    "    \n",
    "    print(f\"üìù Total files: {total_files}\")\n",
    "    print(f\"üìè Total lines of code: {total_lines}\")\n",
    "    print(f\"üìä Average lines per file: {total_lines/total_files if total_files > 0 else 0:.1f}\")\n",
    "\n",
    "print(\"\\nüèÜ Testing completed!\")\n",
    "print(\"üí° Your generated React app is ready for download from Kaggle output!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88feac3",
   "metadata": {},
   "source": [
    "# üéâ Download Your Generated React App\n",
    "\n",
    "Your React application has been generated and is ready for download! \n",
    "\n",
    "## üì• How to Download from Kaggle\n",
    "\n",
    "1. **Check Output Panel**: Look at the right sidebar for \"Output\" section\n",
    "2. **Find Your Project**: Look for `kaggle_generated_app` folder\n",
    "3. **Download**: Click the download button next to your project\n",
    "4. **Extract**: Unzip the downloaded file on your local machine\n",
    "\n",
    "## üìÇ What You'll Get\n",
    "\n",
    "Your downloaded project includes:\n",
    "\n",
    "- ‚úÖ **Complete React App**: TypeScript, modern React hooks\n",
    "- ‚úÖ **Package.json**: All dependencies and scripts configured\n",
    "- ‚úÖ **Build System**: Ready for `npm run build` and deployment\n",
    "- ‚úÖ **Source Code**: Clean, well-organized components\n",
    "- ‚úÖ **Styling**: Modern CSS/styled-components\n",
    "- ‚úÖ **Tests**: Basic test setup included\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "1. **Local Setup**:\n",
    "   ```bash\n",
    "   cd kaggle_generated_app\n",
    "   npm install\n",
    "   npm start\n",
    "   ```\n",
    "\n",
    "2. **Development**: \n",
    "   - Open in VS Code or your preferred editor\n",
    "   - Customize components and styling\n",
    "   - Add additional features\n",
    "\n",
    "3. **Deployment**:\n",
    "   - `npm run build` for production build\n",
    "   - Deploy to Vercel, Netlify, or your preferred platform\n",
    "\n",
    "## üèÜ Kaggle Advantages Used\n",
    "\n",
    "- **‚ö° Powerful GPUs**: Faster model inference than most local setups\n",
    "- **üìä Large Models**: Access to models that might not fit locally  \n",
    "- **‚è±Ô∏è Long Sessions**: 12+ hour sessions for complex projects\n",
    "- **üíæ Fast Storage**: NVMe storage for quick model loading\n",
    "- **üåê Easy Sharing**: Share your notebook with the community\n",
    "\n",
    "## üí° Tips for Future Use\n",
    "\n",
    "- **Model Selection**: Try different models based on available GPU\n",
    "- **Requirements**: More detailed requirements = better results\n",
    "- **Iterations**: Run multiple times with refined requirements\n",
    "- **Community**: Share your generated apps with Kaggle community\n",
    "- **Datasets**: Integrate with Kaggle datasets for data-driven apps\n",
    "\n",
    "## ü§ù Support\n",
    "\n",
    "If you encounter any issues:\n",
    "- Check the Kaggle logs for detailed error messages\n",
    "- Ensure GPU is enabled in notebook settings\n",
    "- Try different model sizes based on available resources\n",
    "- Join the discussion in comments below!\n",
    "\n",
    "---\n",
    "\n",
    "**Happy coding on Kaggle! üèÜüöÄ**\n",
    "\n",
    "*Generated with AI-powered Frontend Generator - Making React development faster and smarter!*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
