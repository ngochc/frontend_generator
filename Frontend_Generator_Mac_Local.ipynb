{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c261eff",
   "metadata": {},
   "source": [
    "# üçé React Frontend Generator - Mac Local (18GB RAM Optimized)\n",
    "\n",
    "**Perfect for your Mac with 18GB RAM! No API costs, complete privacy, offline capability.**\n",
    "\n",
    "This notebook runs the AI-powered React Frontend Generator using **local models only** on your Mac. Optimized specifically for 18GB RAM systems with MPS acceleration support.\n",
    "\n",
    "## üéØ What You'll Get\n",
    "\n",
    "- **Complete React Applications**: Generated using local DeepSeek-Coder models\n",
    "- **Modern TypeScript**: Best practices and clean code\n",
    "- **Production Ready**: Includes package.json, testing, and build configs\n",
    "- **Zero API Costs**: Everything runs locally on your Mac\n",
    "- **Full Privacy**: Your code never leaves your computer\n",
    "- **Offline Capable**: Works without internet (after initial model download)\n",
    "\n",
    "## üçé Mac-Specific Optimizations\n",
    "\n",
    "- **DeepSeek-Coder 6.7B**: Perfect fit for 18GB RAM\n",
    "- **MPS Acceleration**: Uses Mac's Metal Performance Shaders (M1/M2)\n",
    "- **Smart Memory Management**: Conservative 70% memory utilization\n",
    "- **Local Model Caching**: Download once, use forever\n",
    "- **Fast Local Inference**: Often faster than API calls\n",
    "\n",
    "## üìã Requirements\n",
    "\n",
    "- ‚úÖ **Mac with 18GB RAM** (you're all set!)\n",
    "- ‚úÖ **Python 3.8+** with pip\n",
    "- ‚úÖ **5-15GB free storage** (for model caching)\n",
    "- ‚úÖ **macOS 10.15+** (for MPS support on M1/M2)\n",
    "- üö´ **No OpenAI API key needed**\n",
    "\n",
    "Let's build amazing React apps locally on your Mac! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f808e4",
   "metadata": {},
   "source": [
    "## üîß Step 1: Mac Environment Setup\n",
    "\n",
    "Install dependencies optimized for Mac local execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f5e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mac-optimized dependency installation\n",
    "import sys\n",
    "import os\n",
    "import psutil\n",
    "import subprocess\n",
    "\n",
    "print(\"üçé Installing dependencies for Mac local execution...\")\n",
    "\n",
    "# Install core packages with Mac-specific optimizations\n",
    "packages = [\n",
    "    \"vllm\",  # Local model inference\n",
    "    \"torch\",  # PyTorch for Mac MPS\n",
    "    \"transformers\",  # Model handling\n",
    "    \"tqdm\",  # Progress bars\n",
    "    \"psutil\"  # System monitoring\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        print(f\"üì¶ Installing {package}...\")\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], \n",
    "                      check=True, capture_output=True)\n",
    "        print(f\"‚úÖ {package} installed successfully\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to install {package}: {e}\")\n",
    "\n",
    "# Check Mac system resources\n",
    "memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "available_gb = psutil.virtual_memory().available / (1024**3)\n",
    "cpu_count = psutil.cpu_count()\n",
    "\n",
    "print(f\"\\nüíª Mac System Information:\")\n",
    "print(f\"   Total RAM: {memory_gb:.1f}GB\")\n",
    "print(f\"   Available RAM: {available_gb:.1f}GB\")\n",
    "print(f\"   CPU Cores: {cpu_count}\")\n",
    "\n",
    "# Check for Mac-specific features\n",
    "try:\n",
    "    import torch\n",
    "    mps_available = torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False\n",
    "    print(f\"   MPS (Mac GPU): {'‚úÖ Available' if mps_available else '‚ùå Not available'}\")\n",
    "except ImportError:\n",
    "    print(f\"   MPS (Mac GPU): ‚è≥ PyTorch not yet loaded\")\n",
    "\n",
    "if memory_gb >= 16:\n",
    "    print(\"‚úÖ Perfect! Your Mac has excellent RAM for local models\")\n",
    "    recommended_model = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "elif memory_gb >= 12:\n",
    "    print(\"‚úÖ Good! Your Mac can run medium-sized models\")\n",
    "    recommended_model = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Limited RAM - recommend smallest models\")\n",
    "    recommended_model = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
    "\n",
    "print(f\"üéØ Recommended model for your Mac: {recommended_model}\")\n",
    "\n",
    "# Set up local directories\n",
    "WORK_DIR = os.path.expanduser(\"~/Frontend_Generator\")\n",
    "MODELS_DIR = f\"{WORK_DIR}/models\"\n",
    "PROJECTS_DIR = f\"{WORK_DIR}/generated_projects\"\n",
    "\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(PROJECTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"\\nüìÅ Mac workspace initialized:\")\n",
    "print(f\"   Working directory: {WORK_DIR}\")\n",
    "print(f\"   Models cache: {MODELS_DIR}\")\n",
    "print(f\"   Projects: {PROJECTS_DIR}\")\n",
    "\n",
    "# Change to working directory\n",
    "os.chdir(WORK_DIR)\n",
    "print(f\"üìÇ Current directory: {os.getcwd()}\")\n",
    "\n",
    "print(\"\\nüçé Mac environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5443635b",
   "metadata": {},
   "source": [
    "## üì• Step 2: Download Frontend Generator\n",
    "\n",
    "Clone the Frontend Generator repository to your Mac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f239fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Frontend Generator repository\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "REPO_URL = \"https://github.com/ngochc/frontend_generator.git\"\n",
    "REPO_DIR = \"frontend_generator\"\n",
    "\n",
    "print(\"üì• Downloading Frontend Generator repository...\")\n",
    "\n",
    "try:\n",
    "    # Remove existing directory if it exists\n",
    "    if os.path.exists(REPO_DIR):\n",
    "        shutil.rmtree(REPO_DIR)\n",
    "    \n",
    "    # Clone the repository using git\n",
    "    result = subprocess.run(['git', 'clone', REPO_URL, REPO_DIR], \n",
    "                          capture_output=True, text=True, check=True)\n",
    "    \n",
    "    print(\"‚úÖ Repository cloned successfully!\")\n",
    "    print(f\"üìÅ Repository location: {os.path.join(os.getcwd(), REPO_DIR)}\")\n",
    "    \n",
    "    # List key contents\n",
    "    repo_contents = os.listdir(REPO_DIR)\n",
    "    print(f\"üìã Repository contents: {repo_contents}\")\n",
    "    \n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"‚ùå Git clone failed: {e}\")\n",
    "    print(\"üîÑ Trying alternative download method...\")\n",
    "    \n",
    "    # Alternative: Download as ZIP\n",
    "    import urllib.request\n",
    "    import zipfile\n",
    "    \n",
    "    zip_url = f\"{REPO_URL.replace('.git', '')}/archive/main.zip\"\n",
    "    zip_file = \"frontend_generator.zip\"\n",
    "    \n",
    "    try:\n",
    "        print(\"üì¶ Downloading ZIP archive...\")\n",
    "        urllib.request.urlretrieve(zip_url, zip_file)\n",
    "        \n",
    "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall()\n",
    "        \n",
    "        # Rename extracted folder\n",
    "        extracted_folder = \"frontend_generator-main\"\n",
    "        if os.path.exists(extracted_folder):\n",
    "            os.rename(extracted_folder, REPO_DIR)\n",
    "        \n",
    "        os.remove(zip_file)\n",
    "        print(\"‚úÖ Repository downloaded as ZIP successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Download failed: {e}\")\n",
    "        print(\"üí° Please manually download the repository files\")\n",
    "\n",
    "# Verify essential files exist\n",
    "essential_files = [\n",
    "    f\"{REPO_DIR}/codes/1_planning_llm.py\",\n",
    "    f\"{REPO_DIR}/codes/2_analyzing_llm.py\", \n",
    "    f\"{REPO_DIR}/codes/3_coding_llm.py\",\n",
    "    f\"{REPO_DIR}/codes/4_testing_llm.py\",\n",
    "    f\"{REPO_DIR}/codes/utils.py\",\n",
    "    f\"{REPO_DIR}/examples/simple_todo_requirements.md\"\n",
    "]\n",
    "\n",
    "missing_files = [f for f in essential_files if not os.path.exists(f)]\n",
    "if missing_files:\n",
    "    print(f\"‚ö†Ô∏è Missing files: {missing_files}\")\n",
    "    print(\"üí° Please ensure all files are downloaded correctly\")\n",
    "else:\n",
    "    print(\"‚úÖ All essential files found!\")\n",
    "    print(\"üçé Mac repository setup complete!\")\n",
    "\n",
    "# Display available examples\n",
    "examples_dir = f\"{REPO_DIR}/examples\"\n",
    "if os.path.exists(examples_dir):\n",
    "    examples = [f for f in os.listdir(examples_dir) if f.endswith('.md')]\n",
    "    print(f\"\\nüìö Available project examples ({len(examples)}):\")\n",
    "    for example in examples:\n",
    "        print(f\"   üìÑ {example}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Examples directory not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af6944a",
   "metadata": {},
   "source": [
    "## ü§ñ Step 3: Mac-Optimized Local Model Setup\n",
    "\n",
    "Configure vLLM and DeepSeek-Coder model for your 18GB Mac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5787d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mac-optimized vLLM setup for 18GB RAM\n",
    "import torch\n",
    "\n",
    "print(\"üçé Setting up local models for Mac...\")\n",
    "\n",
    "try:\n",
    "    from vllm import LLM, SamplingParams\n",
    "    print(\"‚úÖ vLLM imported successfully\")\n",
    "    \n",
    "    # Check Mac-specific acceleration\n",
    "    mps_available = torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    \n",
    "    print(f\"\\nüîç Mac Acceleration Status:\")\n",
    "    print(f\"   MPS (Mac GPU): {'‚úÖ Available' if mps_available else '‚ùå Not available'}\")\n",
    "    print(f\"   CUDA: {'‚úÖ Available' if cuda_available else '‚ùå Not available'}\")\n",
    "    \n",
    "    # Model selection optimized for 18GB Mac\n",
    "    if memory_gb >= 16:\n",
    "        # Perfect for 18GB Mac\n",
    "        VLLM_MODEL = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "        memory_utilization = 0.7  # Conservative for 18GB\n",
    "        print(f\"üéØ Selected model: {VLLM_MODEL} (optimal for 18GB)\")\n",
    "    else:\n",
    "        # Fallback for lower memory\n",
    "        VLLM_MODEL = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
    "        memory_utilization = 0.8\n",
    "        print(f\"üéØ Selected model: {VLLM_MODEL} (conservative choice)\")\n",
    "    \n",
    "    # Mac-optimized vLLM configuration\n",
    "    vllm_config = {\n",
    "        \"model\": VLLM_MODEL,\n",
    "        \"tensor_parallel_size\": 1,\n",
    "        \"max_model_len\": 2048,  # Reasonable context for Mac\n",
    "        \"swap_space\": 4,  # Allow some swap usage\n",
    "    }\n",
    "    \n",
    "    # Configure device-specific settings\n",
    "    if mps_available:\n",
    "        print(\"üçé Configuring for Mac MPS acceleration\")\n",
    "        # Note: vLLM MPS support varies, may fall back to CPU\n",
    "        vllm_config[\"gpu_memory_utilization\"] = memory_utilization\n",
    "    elif cuda_available:\n",
    "        print(\"üéÆ Configuring for CUDA\")\n",
    "        vllm_config[\"gpu_memory_utilization\"] = memory_utilization\n",
    "    else:\n",
    "        print(\"üñ•Ô∏è Configuring for CPU-only mode\")\n",
    "        vllm_config[\"gpu_memory_utilization\"] = 0.0\n",
    "        # Use smaller model for CPU\n",
    "        if memory_gb < 16:\n",
    "            VLLM_MODEL = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
    "            vllm_config[\"model\"] = VLLM_MODEL\n",
    "    \n",
    "    # Model caching setup\n",
    "    model_cache_path = f\"{MODELS_DIR}/{VLLM_MODEL.replace('/', '_')}\"\n",
    "    \n",
    "    if os.path.exists(model_cache_path):\n",
    "        print(f\"‚úÖ Found cached model at: {model_cache_path}\")\n",
    "        model_location = model_cache_path\n",
    "    else:\n",
    "        print(f\"üì• Model will be downloaded and cached\")\n",
    "        print(f\"üíæ Cache location: {model_cache_path}\")\n",
    "        model_location = VLLM_MODEL\n",
    "        os.makedirs(model_cache_path, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n‚öôÔ∏è Mac vLLM Configuration:\")\n",
    "    for key, value in vllm_config.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    # Optional: Test model loading\n",
    "    TEST_MODEL = False  # Set to True to test model loading now\n",
    "    \n",
    "    if TEST_MODEL:\n",
    "        print(f\"\\nüîÑ Testing model loading on Mac...\")\n",
    "        try:\n",
    "            print(\"Loading model (this may take a few minutes)...\")\n",
    "            llm = LLM(**vllm_config)\n",
    "            print(\"‚úÖ Model loaded successfully on Mac!\")\n",
    "            \n",
    "            # Quick test\n",
    "            sampling_params = SamplingParams(temperature=0.1, max_tokens=50)\n",
    "            test_output = llm.generate([\"// Create a React component\"], sampling_params)\n",
    "            print(\"‚úÖ Test generation successful!\")\n",
    "            print(f\"üìù Sample: {test_output[0].outputs[0].text[:50]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Model test failed: {e}\")\n",
    "            print(\"üí° Model will be loaded during actual generation\")\n",
    "    \n",
    "    print(f\"\\nüçé Mac Model Setup Summary:\")\n",
    "    print(f\"   Model: {VLLM_MODEL}\")\n",
    "    print(f\"   Memory: {memory_gb:.1f}GB total, {memory_utilization*100:.0f}% utilization\")\n",
    "    print(f\"   Acceleration: {'MPS' if mps_available else 'CUDA' if cuda_available else 'CPU'}\")\n",
    "    print(f\"   Cache: {model_cache_path}\")\n",
    "    print(f\"   Ready: ‚úÖ\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ùå vLLM not available\")\n",
    "    print(\"üí° Install with: pip install vllm\")\n",
    "    VLLM_MODEL = None\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model setup failed: {e}\")\n",
    "    VLLM_MODEL = None\n",
    "\n",
    "# Set variables for next steps\n",
    "USE_VLLM = VLLM_MODEL is not None\n",
    "if USE_VLLM:\n",
    "    print(\"\\nüöÄ Ready to generate React apps locally on Mac!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
