{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db8ceee4",
   "metadata": {},
   "source": [
    "# üöÄ React Frontend Generator - Local Models (No API Required)\n",
    "\n",
    "This notebook runs the AI-powered React Frontend Generator using **local models only** - perfect for your Mac with 18GB RAM! No OpenAI API costs, complete privacy, and offline capability.\n",
    "\n",
    "## üéØ What You'll Get\n",
    "\n",
    "- **Complete React Applications**: Generated from your requirements using local AI models\n",
    "- **Modern TypeScript**: Best practices and clean code\n",
    "- **Production Ready**: Includes package.json, testing, and build configs\n",
    "- **Zero API Costs**: Everything runs locally on your machine\n",
    "- **Full Privacy**: Your code never leaves your computer\n",
    "- **Offline Capable**: Works without internet connection (after initial model download)\n",
    "\n",
    "## üçé Perfect for Mac 18GB RAM\n",
    "\n",
    "- **Optimized Models**: DeepSeek-Coder 6.7B works great with 18GB RAM\n",
    "- **MPS Acceleration**: Uses Mac's Metal Performance Shaders if available\n",
    "- **Memory Efficient**: Conservative memory usage with smart caching\n",
    "- **Fast Generation**: Local inference is often faster than API calls\n",
    "\n",
    "## üìã Requirements\n",
    "\n",
    "- **Mac with 18GB RAM** (you're all set!)\n",
    "- **Python 3.8+** with pip\n",
    "- **5-15GB free storage** (for model caching)\n",
    "- **No OpenAI API key needed** üéâ\n",
    "\n",
    "## üîÑ Workflow\n",
    "\n",
    "1. **Setup Environment**: Install vLLM and dependencies\n",
    "2. **Download Project**: Clone Frontend Generator from GitHub  \n",
    "3. **Configure Models**: Set up local DeepSeek-Coder model\n",
    "4. **Generate React App**: Create applications from requirements\n",
    "5. **Local Development**: Complete React project ready to run\n",
    "\n",
    "Let's build amazing React apps locally! üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a0e84",
   "metadata": {},
   "source": [
    "## üîß Step 1: Environment Setup and Dependencies\n",
    "\n",
    "First, let's install all the required packages for the Frontend Generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831153e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for local model inference\n",
    "print(\"üì¶ Installing dependencies for local model inference...\")\n",
    "\n",
    "# Core packages\n",
    "!pip install vllm tqdm pathlib psutil\n",
    "\n",
    "# Optional: OpenAI for backup (can be skipped)\n",
    "!pip install openai\n",
    "\n",
    "# Check platform and system resources\n",
    "import sys\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "# Get system memory\n",
    "memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "print(f\"üíæ Detected {memory_gb:.1f}GB RAM\")\n",
    "\n",
    "if memory_gb >= 16:\n",
    "    print(\"‚úÖ Excellent! Your system has enough RAM for local models\")\n",
    "elif memory_gb >= 12:\n",
    "    print(\"‚úÖ Good! Your system can run smaller local models\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Limited RAM detected - consider using smaller models\")\n",
    "\n",
    "# Platform detection\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"‚úÖ Running on Google Colab\")\n",
    "    PLATFORM = \"colab\"\n",
    "elif 'kaggle' in sys.modules:\n",
    "    print(\"‚úÖ Running on Kaggle\")  \n",
    "    PLATFORM = \"kaggle\"\n",
    "else:\n",
    "    print(\"‚úÖ Running locally (perfect for Mac!)\")\n",
    "    PLATFORM = \"local\"\n",
    "\n",
    "print(\"üì¶ Dependencies installed successfully!\")\n",
    "print(\"üçé Ready for local model inference on Mac!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71057986",
   "metadata": {},
   "source": [
    "## üíæ Step 2: Google Drive Integration\n",
    "\n",
    "Let's mount Google Drive to store our generated projects and models conveniently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5155380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLATFORM == \"colab\":\n",
    "    # Mount Google Drive in Colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Create working directories\n",
    "    DRIVE_PATH = \"/content/drive/MyDrive\"\n",
    "    WORK_DIR = f\"{DRIVE_PATH}/Frontend_Generator\"\n",
    "    MODELS_DIR = f\"{WORK_DIR}/models\"\n",
    "    PROJECTS_DIR = f\"{WORK_DIR}/generated_projects\"\n",
    "    \n",
    "    os.makedirs(WORK_DIR, exist_ok=True)\n",
    "    os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "    os.makedirs(PROJECTS_DIR, exist_ok=True)\n",
    "    \n",
    "    print(f\"‚úÖ Google Drive mounted at: {DRIVE_PATH}\")\n",
    "    print(f\"üìÅ Working directory: {WORK_DIR}\")\n",
    "    \n",
    "elif PLATFORM == \"kaggle\":\n",
    "    # For Kaggle, use local storage\n",
    "    WORK_DIR = \"/kaggle/working/Frontend_Generator\"\n",
    "    MODELS_DIR = f\"{WORK_DIR}/models\"\n",
    "    PROJECTS_DIR = f\"{WORK_DIR}/generated_projects\"\n",
    "    \n",
    "    os.makedirs(WORK_DIR, exist_ok=True)\n",
    "    os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "    os.makedirs(PROJECTS_DIR, exist_ok=True)\n",
    "    \n",
    "    print(f\"‚úÖ Kaggle workspace initialized\")\n",
    "    print(f\"üìÅ Working directory: {WORK_DIR}\")\n",
    "\n",
    "else:\n",
    "    # Default local setup\n",
    "    WORK_DIR = \"./Frontend_Generator\"\n",
    "    MODELS_DIR = f\"{WORK_DIR}/models\"\n",
    "    PROJECTS_DIR = f\"{WORK_DIR}/generated_projects\"\n",
    "    \n",
    "    os.makedirs(WORK_DIR, exist_ok=True)\n",
    "    os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "    os.makedirs(PROJECTS_DIR, exist_ok=True)\n",
    "    \n",
    "    print(f\"‚úÖ Local workspace initialized\")\n",
    "    print(f\"üìÅ Working directory: {WORK_DIR}\")\n",
    "\n",
    "# Change to working directory\n",
    "os.chdir(WORK_DIR)\n",
    "print(f\"üìÇ Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc2492b",
   "metadata": {},
   "source": [
    "## üì• Step 3: Download Frontend Generator from GitHub\n",
    "\n",
    "Let's clone the Frontend Generator repository to get all the necessary code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bf4d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the frontend generator repository\n",
    "import subprocess\n",
    "\n",
    "REPO_URL = \"https://github.com/ngochc/frontend_generator.git\"  # Replace with your repo URL\n",
    "REPO_DIR = \"frontend_generator\"\n",
    "\n",
    "try:\n",
    "    # Remove existing directory if it exists\n",
    "    if os.path.exists(REPO_DIR):\n",
    "        import shutil\n",
    "        shutil.rmtree(REPO_DIR)\n",
    "    \n",
    "    # Clone the repository\n",
    "    result = subprocess.run(['git', 'clone', REPO_URL, REPO_DIR], \n",
    "                          capture_output=True, text=True, check=True)\n",
    "    \n",
    "    print(\"‚úÖ Repository cloned successfully!\")\n",
    "    print(f\"üìÅ Repository directory: {os.path.join(os.getcwd(), REPO_DIR)}\")\n",
    "    \n",
    "    # List the contents\n",
    "    repo_contents = os.listdir(REPO_DIR)\n",
    "    print(f\"üìã Repository contents: {repo_contents}\")\n",
    "    \n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"‚ùå Git clone failed: {e}\")\n",
    "    print(\"üîÑ Trying alternative download method...\")\n",
    "    \n",
    "    # Alternative: Download as ZIP\n",
    "    import urllib.request\n",
    "    import zipfile\n",
    "    \n",
    "    zip_url = f\"{REPO_URL.replace('.git', '')}/archive/main.zip\"\n",
    "    zip_file = \"frontend_generator.zip\"\n",
    "    \n",
    "    try:\n",
    "        urllib.request.urlretrieve(zip_url, zip_file)\n",
    "        \n",
    "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall()\n",
    "        \n",
    "        # Rename extracted folder\n",
    "        extracted_folder = \"frontend_generator-main\"\n",
    "        if os.path.exists(extracted_folder):\n",
    "            os.rename(extracted_folder, REPO_DIR)\n",
    "        \n",
    "        os.remove(zip_file)\n",
    "        print(\"‚úÖ Repository downloaded as ZIP successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Download failed: {e}\")\n",
    "        print(\"Please manually upload the repository files.\")\n",
    "\n",
    "# Verify key files exist\n",
    "key_files = [\n",
    "    f\"{REPO_DIR}/codes/1_planning_llm.py\",\n",
    "    f\"{REPO_DIR}/codes/2_analyzing_llm.py\", \n",
    "    f\"{REPO_DIR}/codes/3_coding_llm.py\",\n",
    "    f\"{REPO_DIR}/codes/4_testing_llm.py\",\n",
    "    f\"{REPO_DIR}/codes/utils.py\",\n",
    "    f\"{REPO_DIR}/examples/simple_todo_requirements.md\"\n",
    "]\n",
    "\n",
    "missing_files = [f for f in key_files if not os.path.exists(f)]\n",
    "if missing_files:\n",
    "    print(f\"‚ö†Ô∏è Missing files: {missing_files}\")\n",
    "else:\n",
    "    print(\"‚úÖ All key files found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfae11b",
   "metadata": {},
   "source": [
    "## üîë Step 4: API Configuration (Optional - Skip for Local Models)\n",
    "\n",
    "Since we're using local models with vLLM, you can skip OpenAI API setup. This section is only needed if you want to use OpenAI models as backup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1eb625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: OpenAI API setup (only needed as backup to local models)\n",
    "SETUP_OPENAI = False  # Set to True if you want OpenAI as backup\n",
    "\n",
    "if SETUP_OPENAI:\n",
    "    import getpass\n",
    "    from openai import OpenAI\n",
    "\n",
    "    # Get OpenAI API key securely\n",
    "    if PLATFORM == \"colab\":\n",
    "        from google.colab import userdata\n",
    "        try:\n",
    "            # Try to get from Colab secrets first\n",
    "            OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "            print(\"‚úÖ API key loaded from Colab secrets\")\n",
    "        except:\n",
    "            # Fall back to manual input\n",
    "            OPENAI_API_KEY = getpass.getpass(\"üîë Enter your OpenAI API key: \")\n",
    "            print(\"‚úÖ API key entered manually\")\n",
    "    else:\n",
    "        # Manual input for other platforms\n",
    "        OPENAI_API_KEY = getpass.getpass(\"üîë Enter your OpenAI API key: \")\n",
    "        print(\"‚úÖ API key entered manually\")\n",
    "\n",
    "    # Set environment variable\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "    # Test API connection\n",
    "    try:\n",
    "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "        # Test with a simple request\n",
    "        response = client.models.list()\n",
    "        print(\"‚úÖ OpenAI API connection successful!\")\n",
    "        print(f\"ü§ñ Available models: {len(list(response.data))} models found\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå API connection failed: {e}\")\n",
    "        print(\"Please check your API key and try again.\")\n",
    "\n",
    "    # Recommended models for the generator\n",
    "    RECOMMENDED_MODELS = [\n",
    "        \"gpt-4o\",           # Latest GPT-4\n",
    "        \"gpt-4\",            # Standard GPT-4\n",
    "        \"gpt-4-turbo\",      # GPT-4 Turbo\n",
    "        \"o3-mini\",          # O3 Mini (cost-effective)\n",
    "        \"gpt-3.5-turbo\"     # Most affordable\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nüí° Recommended OpenAI models for Frontend Generator:\")\n",
    "    for model in RECOMMENDED_MODELS:\n",
    "        print(f\"   - {model}\")\n",
    "\n",
    "    # Default model selection\n",
    "    DEFAULT_MODEL = \"o3-mini\"  # Cost-effective choice\n",
    "    print(f\"\\nüéØ Using default OpenAI model: {DEFAULT_MODEL}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è OpenAI API setup skipped - using local models only\")\n",
    "    print(\"üí° This saves costs and provides privacy!\")\n",
    "    print(\"üîß All generation will use local vLLM models\")\n",
    "    \n",
    "    # Set a placeholder for compatibility\n",
    "    DEFAULT_MODEL = \"local-vllm-model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3596107f",
   "metadata": {},
   "source": [
    "## ü§ñ Step 4b: Local Model Setup with vLLM (Primary Method)\n",
    "\n",
    "Set up vLLM for local model inference - perfect for your Mac with 18GB RAM! No API costs, complete privacy, and offline capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8663f39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vLLM Setup for Local Model Inference (Optimized for Mac 18GB RAM)\n",
    "USE_VLLM = True  # Enabled by default for local execution\n",
    "\n",
    "if USE_VLLM:\n",
    "    try:\n",
    "        from vllm import LLM, SamplingParams\n",
    "        \n",
    "        # Check system resources\n",
    "        import torch\n",
    "        import psutil\n",
    "        \n",
    "        # Get system memory\n",
    "        memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "        print(f\"üíæ System Memory: {memory_gb:.1f}GB\")\n",
    "        \n",
    "        # Check if CUDA/MPS is available (for Mac M1/M2)\n",
    "        gpu_available = torch.cuda.is_available()\n",
    "        mps_available = torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False\n",
    "        \n",
    "        print(f\"üîç CUDA Available: {gpu_available}\")\n",
    "        print(f\"üçé MPS (Mac GPU) Available: {mps_available}\")\n",
    "        \n",
    "        if gpu_available:\n",
    "            gpu_count = torch.cuda.device_count()\n",
    "            print(f\"üéÆ GPU Count: {gpu_count}\")\n",
    "            for i in range(gpu_count):\n",
    "                gpu_name = torch.cuda.get_device_name(i)\n",
    "                gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "                print(f\"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)\")\n",
    "        \n",
    "        # Model recommendations optimized for 18GB RAM Mac\n",
    "        MODEL_RECOMMENDATIONS = {\n",
    "            \"recommended_18gb\": {\n",
    "                \"model\": \"deepseek-ai/deepseek-coder-6.7b-instruct\",\n",
    "                \"memory_req\": \"8-12GB\",\n",
    "                \"description\": \"Best choice for 18GB Mac - good performance\"\n",
    "            },\n",
    "            \"safe_choice\": {\n",
    "                \"model\": \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
    "                \"memory_req\": \"3-4GB\",\n",
    "                \"description\": \"Conservative choice, leaves plenty of memory\"\n",
    "            },\n",
    "            \"alternative\": {\n",
    "                \"model\": \"microsoft/DialoGPT-medium\",\n",
    "                \"memory_req\": \"2-3GB\", \n",
    "                \"description\": \"Lightweight alternative for basic tasks\"\n",
    "            },\n",
    "            \"cpu_optimized\": {\n",
    "                \"model\": \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
    "                \"memory_req\": \"4-6GB (CPU)\",\n",
    "                \"description\": \"CPU-only mode for systems without GPU acceleration\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(\"\\nü§ñ vLLM Model Recommendations for 18GB Mac:\")\n",
    "        for config, info in MODEL_RECOMMENDATIONS.items():\n",
    "            print(f\"   {config.replace('_', ' ').title()}:\")\n",
    "            print(f\"      Model: {info['model']}\")\n",
    "            print(f\"      Memory: {info['memory_req']}\")\n",
    "            print(f\"      Use: {info['description']}\")\n",
    "        \n",
    "        # Select optimal model for 18GB Mac\n",
    "        VLLM_MODEL = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "        \n",
    "        # Configure vLLM for Mac optimization\n",
    "        vllm_config = {\n",
    "            \"model\": VLLM_MODEL,\n",
    "            \"tensor_parallel_size\": 1,\n",
    "            \"gpu_memory_utilization\": 0.7,  # Conservative for 18GB system\n",
    "            \"max_model_len\": 2048,  # Reasonable context length\n",
    "            \"swap_space\": 4,  # Use some swap if needed\n",
    "        }\n",
    "        \n",
    "        # Adjust for Mac MPS or CPU-only\n",
    "        if mps_available:\n",
    "            print(\"üçé Using Mac MPS (Metal Performance Shaders)\")\n",
    "            vllm_config[\"device\"] = \"mps\"\n",
    "        elif not gpu_available:\n",
    "            print(\"üñ•Ô∏è Using CPU-only mode\")\n",
    "            vllm_config[\"gpu_memory_utilization\"] = 0.0\n",
    "            # Use smaller model for CPU\n",
    "            VLLM_MODEL = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
    "            vllm_config[\"model\"] = VLLM_MODEL\n",
    "        \n",
    "        # Check if model is cached in models directory\n",
    "        model_cache_path = f\"{MODELS_DIR}/{VLLM_MODEL.replace('/', '_')}\"\n",
    "        \n",
    "        if os.path.exists(model_cache_path):\n",
    "            print(f\"\\n‚úÖ Found cached model at: {model_cache_path}\")\n",
    "            VLLM_MODEL_PATH = model_cache_path\n",
    "        else:\n",
    "            print(f\"\\nüì• Model will be downloaded first time\")\n",
    "            print(f\"üí° Model will be cached for future use\")\n",
    "            VLLM_MODEL_PATH = VLLM_MODEL\n",
    "            \n",
    "            # Create cache directory\n",
    "            os.makedirs(model_cache_path, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nüéØ Selected model: {VLLM_MODEL}\")\n",
    "        print(f\"üíæ Model cache: {model_cache_path}\")\n",
    "        print(f\"‚öôÔ∏è Configuration:\")\n",
    "        for key, value in vllm_config.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "            \n",
    "        # Test model loading (optional)\n",
    "        TEST_MODEL_LOADING = False  # Set to True to test model loading\n",
    "        \n",
    "        if TEST_MODEL_LOADING:\n",
    "            print(f\"\\nüîÑ Testing model loading...\")\n",
    "            try:\n",
    "                # Initialize LLM with optimized settings\n",
    "                llm = LLM(**vllm_config)\n",
    "                print(\"‚úÖ Model loaded successfully!\")\n",
    "                \n",
    "                # Quick test generation\n",
    "                sampling_params = SamplingParams(\n",
    "                    temperature=0.1,\n",
    "                    top_p=0.95,\n",
    "                    max_tokens=50\n",
    "                )\n",
    "                \n",
    "                test_prompt = \"// Create a simple React component\"\n",
    "                outputs = llm.generate([test_prompt], sampling_params)\n",
    "                print(\"‚úÖ Test generation successful!\")\n",
    "                print(f\"üìù Sample output: {outputs[0].outputs[0].text[:100]}...\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Model loading test failed: {e}\")\n",
    "                print(\"üí° Model will be loaded during actual generation\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ùå vLLM not installed. Please install with: pip install vllm\")\n",
    "        print(\"üí° Falling back to basic configuration\")\n",
    "        USE_VLLM = False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå vLLM setup failed: {e}\")\n",
    "        print(\"üí° Check if your system supports vLLM\")\n",
    "        USE_VLLM = False\n",
    "\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è vLLM disabled - using OpenAI API\")\n",
    "    print(\"üí° To use local models, set USE_VLLM = True above\")\n",
    "\n",
    "# Mac-specific optimizations\n",
    "if USE_VLLM:\n",
    "    print(f\"\\nüçé Mac 18GB RAM Optimizations:\")\n",
    "    print(f\"   ‚úÖ Using optimized model for 18GB memory\")\n",
    "    print(f\"   ‚úÖ Conservative memory utilization (70%)\")\n",
    "    print(f\"   ‚úÖ Reasonable context length (2048 tokens)\")\n",
    "    print(f\"   ‚úÖ Model caching enabled\")\n",
    "    print(f\"   ‚úÖ {'MPS acceleration' if mps_available else 'CPU fallback'}\")\n",
    "\n",
    "# Summary of configuration\n",
    "print(f\"\\nüìã Configuration Summary:\")\n",
    "print(f\"   Platform: {PLATFORM}\")\n",
    "print(f\"   Working Directory: {WORK_DIR}\")\n",
    "print(f\"   Using vLLM: {USE_VLLM}\")\n",
    "print(f\"   Model: {VLLM_MODEL if USE_VLLM else 'OpenAI API'}\")\n",
    "print(f\"   Memory Available: {memory_gb:.1f}GB\")\n",
    "print(f\"   Acceleration: {'MPS' if mps_available else 'CUDA' if gpu_available else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd313235",
   "metadata": {},
   "source": [
    "## üìã Step 5: Project Requirements Setup\n",
    "\n",
    "Create or upload your project requirements. The Frontend Generator supports markdown, JSON, or text formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a019c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Use provided example requirements\n",
    "EXAMPLE_REQUIREMENTS = {\n",
    "    \"simple_todo\": f\"{REPO_DIR}/examples/simple_todo_requirements.md\",\n",
    "    \"dashboard\": f\"{REPO_DIR}/examples/dashboard_requirements.md\", \n",
    "    \"healthcare\": f\"{REPO_DIR}/examples/healthcare_management_requirements.md\",\n",
    "    \"trading\": f\"{REPO_DIR}/examples/trading_platform_requirements.md\",\n",
    "    \"enterprise\": f\"{REPO_DIR}/examples/enterprise_project_platform_requirements.md\"\n",
    "}\n",
    "\n",
    "print(\"üìö Available Example Requirements:\")\n",
    "for name, path in EXAMPLE_REQUIREMENTS.items():\n",
    "    if os.path.exists(path):\n",
    "        print(f\"   ‚úÖ {name}: {path}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {name}: {path} (not found)\")\n",
    "\n",
    "# Option 2: Create custom requirements\n",
    "def create_custom_requirements():\n",
    "    \"\"\"Create a custom requirements file\"\"\"\n",
    "    \n",
    "    print(\"\\nüìù Create Custom Requirements\")\n",
    "    print(\"Enter your project requirements (press Enter twice to finish):\")\n",
    "    \n",
    "    lines = []\n",
    "    while True:\n",
    "        try:\n",
    "            line = input()\n",
    "            if line == \"\" and len(lines) > 0 and lines[-1] == \"\":\n",
    "                break\n",
    "            lines.append(line)\n",
    "        except EOFError:\n",
    "            break\n",
    "    \n",
    "    # Remove the last empty line\n",
    "    if lines and lines[-1] == \"\":\n",
    "        lines.pop()\n",
    "    \n",
    "    requirements_content = \"\\n\".join(lines)\n",
    "    \n",
    "    # Save to file\n",
    "    custom_requirements_path = \"custom_requirements.md\"\n",
    "    with open(custom_requirements_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(requirements_content)\n",
    "    \n",
    "    print(f\"‚úÖ Custom requirements saved to: {custom_requirements_path}\")\n",
    "    return custom_requirements_path\n",
    "\n",
    "# Mac-optimized defaults: Start with simple project for testing\n",
    "SELECTED_REQUIREMENTS = EXAMPLE_REQUIREMENTS[\"simple_todo\"]\n",
    "PROJECT_NAME = \"MyTodoApp\"\n",
    "\n",
    "print(f\"\\nüéØ Using requirements: {SELECTED_REQUIREMENTS}\")\n",
    "print(f\"üìù Project name: {PROJECT_NAME}\")\n",
    "print(f\"üçé Optimized for Mac 18GB RAM - starting with simple project\")\n",
    "\n",
    "# Validate requirements file\n",
    "if os.path.exists(SELECTED_REQUIREMENTS):\n",
    "    with open(SELECTED_REQUIREMENTS, 'r', encoding='utf-8') as f:\n",
    "        requirements_content = f.read()\n",
    "    \n",
    "    print(f\"‚úÖ Requirements file found ({len(requirements_content)} characters)\")\n",
    "    print(\"üìÑ Requirements preview:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(requirements_content[:500] + (\"...\" if len(requirements_content) > 500 else \"\"))\n",
    "    print(\"-\" * 50)\n",
    "else:\n",
    "    print(f\"‚ùå Requirements file not found: {SELECTED_REQUIREMENTS}\")\n",
    "    print(\"üí° You can create custom requirements by running: create_custom_requirements()\")\n",
    "\n",
    "# Output directories - optimized for local storage\n",
    "if PLATFORM == \"other\":\n",
    "    # Local Mac setup\n",
    "    OUTPUT_DIR = f\"{PROJECTS_DIR}/{PROJECT_NAME}_output\"\n",
    "    OUTPUT_REPO_DIR = f\"{PROJECTS_DIR}/{PROJECT_NAME}_frontend\"\n",
    "else:\n",
    "    # Cloud setup\n",
    "    OUTPUT_DIR = f\"{PROJECTS_DIR}/{PROJECT_NAME}_output\"\n",
    "    OUTPUT_REPO_DIR = f\"{PROJECTS_DIR}/{PROJECT_NAME}_frontend\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_REPO_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"\\nüìÅ Output directories:\")\n",
    "print(f\"   Logs: {OUTPUT_DIR}\")\n",
    "print(f\"   React App: {OUTPUT_REPO_DIR}\")\n",
    "\n",
    "# Mac-specific recommendations\n",
    "print(f\"\\nüçé Mac Usage Tips:\")\n",
    "print(f\"   üí° Start with simple_todo to test your setup\")\n",
    "print(f\"   ‚ö° Generated app will be fully local - no API costs!\")\n",
    "print(f\"   üîí Complete privacy - your code never leaves your Mac\")\n",
    "print(f\"   üì± Perfect for prototyping and learning React patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fee4838",
   "metadata": {},
   "source": [
    "## üöÄ Step 6: Run Frontend Generator\n",
    "\n",
    "Now let's generate a complete React application using the AI-powered pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c86ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frontend Generator Execution\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Choose between OpenAI and vLLM\n",
    "USE_LLM_SCRIPTS = USE_VLLM  # Use vLLM scripts if vLLM is enabled\n",
    "\n",
    "# Change to the repository directory\n",
    "os.chdir(f\"{WORK_DIR}/{REPO_DIR}\")\n",
    "\n",
    "print(f\"üéØ Generating React application: {PROJECT_NAME}\")\n",
    "print(f\"üìù Requirements: {SELECTED_REQUIREMENTS}\")\n",
    "print(f\"ü§ñ Using {'vLLM' if USE_LLM_SCRIPTS else 'OpenAI'} models\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Execute the three-stage pipeline\n",
    "stages = [\n",
    "    {\n",
    "        \"name\": \"Planning\",\n",
    "        \"script\": \"1_planning_llm.py\" if USE_LLM_SCRIPTS else \"1_planning.py\",\n",
    "        \"description\": \"üèóÔ∏è Architectural planning and technology selection\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Analysis\", \n",
    "        \"script\": \"2_analyzing_llm.py\" if USE_LLM_SCRIPTS else \"2_analyzing.py\",\n",
    "        \"description\": \"üîç Detailed component design and specifications\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Coding\",\n",
    "        \"script\": \"3_coding_llm.py\" if USE_LLM_SCRIPTS else \"3_coding.py\", \n",
    "        \"description\": \"‚öõÔ∏è React component code generation\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Common arguments\n",
    "base_args = [\n",
    "    \"--project_name\", PROJECT_NAME,\n",
    "    \"--requirements_path\", SELECTED_REQUIREMENTS,\n",
    "    \"--output_dir\", OUTPUT_DIR,\n",
    "    \"--output_repo_dir\", OUTPUT_REPO_DIR\n",
    "]\n",
    "\n",
    "if not USE_LLM_SCRIPTS:\n",
    "    base_args.extend([\"--gpt_version\", DEFAULT_MODEL])\n",
    "\n",
    "# Execute each stage\n",
    "total_start_time = time.time()\n",
    "\n",
    "for i, stage in enumerate(stages, 1):\n",
    "    print(f\"\\n{stage['description']}\")\n",
    "    print(f\"Stage {i}/3: {stage['name']}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Build command\n",
    "    cmd = [sys.executable, f\"codes/{stage['script']}\"] + base_args\n",
    "    \n",
    "    # Add model-specific arguments for vLLM\n",
    "    if USE_LLM_SCRIPTS and 'VLLM_MODEL' in globals():\n",
    "        cmd.extend([\"--model_path\", VLLM_MODEL])\n",
    "    \n",
    "    print(f\"üîß Command: {' '.join(cmd)}\")\n",
    "    \n",
    "    try:\n",
    "        stage_start_time = time.time()\n",
    "        \n",
    "        # Execute stage\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.getcwd())\n",
    "        \n",
    "        stage_time = time.time() - stage_start_time\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ {stage['name']} completed successfully! ({stage_time:.1f}s)\")\n",
    "            \n",
    "            # Show last few lines of output\n",
    "            if result.stdout:\n",
    "                output_lines = result.stdout.strip().split('\\n')\n",
    "                print(\"üìÑ Output preview:\")\n",
    "                for line in output_lines[-3:]:\n",
    "                    print(f\"   {line}\")\n",
    "        else:\n",
    "            print(f\"‚ùå {stage['name']} failed!\")\n",
    "            print(f\"Error: {result.stderr}\")\n",
    "            print(f\"Output: {result.stdout}\")\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error executing {stage['name']}: {e}\")\n",
    "        break\n",
    "\n",
    "total_time = time.time() - total_start_time\n",
    "print(f\"\\nüéâ Frontend generation completed in {total_time:.1f} seconds!\")\n",
    "\n",
    "# Verify generated files\n",
    "if os.path.exists(OUTPUT_REPO_DIR):\n",
    "    generated_files = []\n",
    "    for root, dirs, files in os.walk(OUTPUT_REPO_DIR):\n",
    "        for file in files:\n",
    "            if file.endswith(('.tsx', '.ts', '.js', '.jsx', '.json', '.css', '.html')):\n",
    "                rel_path = os.path.relpath(os.path.join(root, file), OUTPUT_REPO_DIR)\n",
    "                generated_files.append(rel_path)\n",
    "    \n",
    "    print(f\"\\nüìÅ Generated {len(generated_files)} files:\")\n",
    "    for file in sorted(generated_files)[:10]:  # Show first 10 files\n",
    "        print(f\"   ‚úÖ {file}\")\n",
    "    \n",
    "    if len(generated_files) > 10:\n",
    "        print(f\"   ... and {len(generated_files) - 10} more files\")\n",
    "        \n",
    "    # Check for key files\n",
    "    key_files = ['package.json', 'src/App.tsx', 'src/index.tsx', 'public/index.html']\n",
    "    missing_key_files = [f for f in key_files if not os.path.exists(os.path.join(OUTPUT_REPO_DIR, f))]\n",
    "    \n",
    "    if missing_key_files:\n",
    "        print(f\"‚ö†Ô∏è Missing key files: {missing_key_files}\")\n",
    "    else:\n",
    "        print(\"‚úÖ All key React files generated successfully!\")\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ùå Output directory not found: {OUTPUT_REPO_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1658f9a",
   "metadata": {},
   "source": [
    "## üß™ Step 6b: Generate Tests (Optional)\n",
    "\n",
    "Generate comprehensive test suites for your React components using AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dc53ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Generate tests for the React components\n",
    "GENERATE_TESTS = True  # Set to False to skip test generation\n",
    "\n",
    "if GENERATE_TESTS and os.path.exists(OUTPUT_REPO_DIR):\n",
    "    print(\"üß™ Generating comprehensive test suite...\")\n",
    "    \n",
    "    # Choose test generation script\n",
    "    test_script = \"4_testing_llm.py\" if USE_LLM_SCRIPTS else \"4_testing.py\"\n",
    "    \n",
    "    # Test generation arguments\n",
    "    test_args = [\n",
    "        sys.executable, f\"codes/{test_script}\",\n",
    "        \"--project_name\", PROJECT_NAME,\n",
    "        \"--project_path\", OUTPUT_REPO_DIR,\n",
    "        \"--requirements_path\", SELECTED_REQUIREMENTS,\n",
    "        \"--test_types\", \"unit,integration\",\n",
    "        \"--test_framework\", \"jest\"\n",
    "    ]\n",
    "    \n",
    "    if not USE_LLM_SCRIPTS:\n",
    "        test_args.extend([\"--gpt_version\", DEFAULT_MODEL])\n",
    "    elif 'VLLM_MODEL' in globals():\n",
    "        test_args.extend([\"--model_path\", VLLM_MODEL])\n",
    "    \n",
    "    try:\n",
    "        print(f\"üîß Running: {' '.join(test_args)}\")\n",
    "        \n",
    "        test_start_time = time.time()\n",
    "        test_result = subprocess.run(test_args, capture_output=True, text=True, cwd=os.getcwd())\n",
    "        test_time = time.time() - test_start_time\n",
    "        \n",
    "        if test_result.returncode == 0:\n",
    "            print(f\"‚úÖ Test generation completed! ({test_time:.1f}s)\")\n",
    "            \n",
    "            # Check for generated test files\n",
    "            test_files = []\n",
    "            for root, dirs, files in os.walk(OUTPUT_REPO_DIR):\n",
    "                for file in files:\n",
    "                    if file.endswith(('.test.tsx', '.test.ts', '.test.js', '.spec.tsx', '.spec.ts', '.spec.js')):\n",
    "                        rel_path = os.path.relpath(os.path.join(root, file), OUTPUT_REPO_DIR)\n",
    "                        test_files.append(rel_path)\n",
    "            \n",
    "            if test_files:\n",
    "                print(f\"üìù Generated {len(test_files)} test files:\")\n",
    "                for test_file in test_files:\n",
    "                    print(f\"   ‚úÖ {test_file}\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No test files found in output\")\n",
    "                \n",
    "        else:\n",
    "            print(\"‚ùå Test generation failed!\")\n",
    "            print(f\"Error: {test_result.stderr}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating tests: {e}\")\n",
    "\n",
    "else:\n",
    "    if not GENERATE_TESTS:\n",
    "        print(\"‚ÑπÔ∏è Test generation skipped (GENERATE_TESTS = False)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Cannot generate tests - React project not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c02e22a",
   "metadata": {},
   "source": [
    "## üì¶ Step 7: Download Generated Project\n",
    "\n",
    "Package and download your generated React application for local development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd712a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package the generated React project\n",
    "import zipfile\n",
    "import shutil\n",
    "from IPython.display import FileLink\n",
    "\n",
    "if os.path.exists(OUTPUT_REPO_DIR):\n",
    "    # Create ZIP archive\n",
    "    zip_filename = f\"{PROJECT_NAME}_frontend.zip\"\n",
    "    zip_path = os.path.join(WORK_DIR, zip_filename)\n",
    "    \n",
    "    print(f\"üì¶ Creating ZIP archive: {zip_filename}\")\n",
    "    \n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            for root, dirs, files in os.walk(OUTPUT_REPO_DIR):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arc_path = os.path.relpath(file_path, OUTPUT_REPO_DIR)\n",
    "                    zipf.write(file_path, arc_path)\n",
    "        \n",
    "        zip_size = os.path.getsize(zip_path) / (1024 * 1024)  # Size in MB\n",
    "        print(f\"‚úÖ ZIP created successfully! Size: {zip_size:.1f}MB\")\n",
    "        \n",
    "        # Create download link for Colab/Kaggle\n",
    "        if PLATFORM == \"colab\":\n",
    "            from google.colab import files\n",
    "            print(\"üì• Downloading project...\")\n",
    "            files.download(zip_path)\n",
    "            print(\"‚úÖ Download started!\")\n",
    "            \n",
    "        elif PLATFORM == \"kaggle\":\n",
    "            # Copy to Kaggle output for download\n",
    "            kaggle_output = \"/kaggle/working\"\n",
    "            kaggle_zip = os.path.join(kaggle_output, zip_filename)\n",
    "            shutil.copy2(zip_path, kaggle_zip)\n",
    "            print(f\"‚úÖ Project saved to Kaggle output: {kaggle_zip}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"‚úÖ Project packaged at: {zip_path}\")\n",
    "            print(\"üí° You can download it manually from the file browser\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating ZIP: {e}\")\n",
    "\n",
    "    # Create a summary report\n",
    "    summary_file = os.path.join(WORK_DIR, f\"{PROJECT_NAME}_summary.md\")\n",
    "    \n",
    "    try:\n",
    "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"# {PROJECT_NAME} - Generation Summary\\n\\n\")\n",
    "            f.write(f\"Generated on: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Platform: {PLATFORM.title()}\\n\")\n",
    "            f.write(f\"Model: {'vLLM' if USE_LLM_SCRIPTS else 'OpenAI'} ({DEFAULT_MODEL if not USE_LLM_SCRIPTS else VLLM_MODEL})\\n\\n\")\n",
    "            \n",
    "            f.write(f\"## Project Structure\\n\\n\")\n",
    "            f.write(\"```\\n\")\n",
    "            \n",
    "            # Generate project structure\n",
    "            for root, dirs, files in os.walk(OUTPUT_REPO_DIR):\n",
    "                level = root.replace(OUTPUT_REPO_DIR, '').count(os.sep)\n",
    "                indent = ' ' * 2 * level\n",
    "                f.write(f\"{indent}{os.path.basename(root)}/\\n\")\n",
    "                \n",
    "                subindent = ' ' * 2 * (level + 1)\n",
    "                for file in files[:5]:  # Limit to first 5 files per directory\n",
    "                    f.write(f\"{subindent}{file}\\n\")\n",
    "                if len(files) > 5:\n",
    "                    f.write(f\"{subindent}... and {len(files) - 5} more files\\n\")\n",
    "                    \n",
    "            f.write(\"```\\n\\n\")\n",
    "            \n",
    "            f.write(f\"## Getting Started\\n\\n\")\n",
    "            f.write(f\"1. Extract the ZIP file\\n\")\n",
    "            f.write(f\"2. Navigate to the project directory:\\n\")\n",
    "            f.write(f\"   ```bash\\n\")\n",
    "            f.write(f\"   cd {PROJECT_NAME}_frontend\\n\")\n",
    "            f.write(f\"   ```\\n\")\n",
    "            f.write(f\"3. Install dependencies:\\n\")\n",
    "            f.write(f\"   ```bash\\n\")\n",
    "            f.write(f\"   npm install\\n\")\n",
    "            f.write(f\"   ```\\n\")\n",
    "            f.write(f\"4. Start development server:\\n\")\n",
    "            f.write(f\"   ```bash\\n\")\n",
    "            f.write(f\"   npm start\\n\")\n",
    "            f.write(f\"   ```\\n\")\n",
    "            f.write(f\"5. Open http://localhost:3000 in your browser\\n\\n\")\n",
    "            \n",
    "            f.write(f\"## Additional Commands\\n\\n\")\n",
    "            f.write(f\"- `npm test` - Run tests\\n\")\n",
    "            f.write(f\"- `npm run build` - Build for production\\n\")\n",
    "            f.write(f\"- `npm run eject` - Eject from Create React App\\n\\n\")\n",
    "            \n",
    "            f.write(f\"Generated by AI-Powered Frontend Generator üöÄ\\n\")\n",
    "        \n",
    "        print(f\"üìÑ Summary created: {summary_file}\")\n",
    "        \n",
    "        if PLATFORM == \"colab\":\n",
    "            files.download(summary_file)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error creating summary: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No generated project found to package\")\n",
    "\n",
    "# Final status\n",
    "print(f\"\\nüéâ Frontend Generator Complete!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üì± Project: {PROJECT_NAME}\")\n",
    "print(f\"üìÅ Location: {OUTPUT_REPO_DIR}\")\n",
    "print(f\"üíæ Archive: {zip_filename if 'zip_filename' in locals() else 'Not created'}\")\n",
    "print(f\"ü§ñ Model: {'vLLM' if USE_LLM_SCRIPTS else 'OpenAI'}\")\n",
    "print(f\"‚è±Ô∏è Total time: {total_time:.1f}s\" if 'total_time' in locals() else \"\")\n",
    "print(\"\\nüöÄ Your React application is ready for development!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e024005b",
   "metadata": {},
   "source": [
    "## üîß Troubleshooting & Next Steps\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "**API Errors:**\n",
    "- Check your OpenAI API key\n",
    "- Verify you have sufficient API credits\n",
    "- Try a different model (e.g., `gpt-3.5-turbo` for lower cost)\n",
    "\n",
    "**Memory Issues with vLLM:**\n",
    "- Use smaller models (`deepseek-coder-1.3b-instruct`)\n",
    "- Reduce `gpu_memory_utilization` parameter\n",
    "- Switch to OpenAI API if GPU memory is limited\n",
    "\n",
    "**Missing Files:**\n",
    "- Ensure the GitHub repository was cloned correctly\n",
    "- Check that all example requirements files exist\n",
    "- Verify output directories are writable\n",
    "\n",
    "### Next Steps After Download\n",
    "\n",
    "1. **Local Development:**\n",
    "   ```bash\n",
    "   cd MyTodoApp_frontend\n",
    "   npm install\n",
    "   npm start\n",
    "   ```\n",
    "\n",
    "2. **Deploy to Production:**\n",
    "   - Vercel: `vercel --prod`\n",
    "   - Netlify: `netlify deploy --prod`\n",
    "   - Firebase: `firebase deploy`\n",
    "\n",
    "3. **Add Features:**\n",
    "   - Modify generated components\n",
    "   - Add new pages and functionality\n",
    "   - Integrate with APIs and databases\n",
    "\n",
    "4. **Testing:**\n",
    "   ```bash\n",
    "   npm test              # Run unit tests\n",
    "   npm run test:coverage # Check coverage\n",
    "   ```\n",
    "\n",
    "### Customization Options\n",
    "\n",
    "- **Change Project Type:** Modify `SELECTED_REQUIREMENTS` variable\n",
    "- **Use Different Models:** Update `DEFAULT_MODEL` or `VLLM_MODEL`\n",
    "- **Custom Requirements:** Use `create_custom_requirements()` function\n",
    "- **Enable vLLM:** Set `USE_VLLM = True` for local inference\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "- **Cost Optimization:** Use `o3-mini` or `gpt-3.5-turbo` for lower costs\n",
    "- **Speed:** Use vLLM with local models for faster generation\n",
    "- **Quality:** Use `gpt-4` or larger models for better code quality\n",
    "\n",
    "üéâ **Happy Coding!** Your AI-generated React app is ready for development!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
